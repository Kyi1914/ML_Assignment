{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch - CNN\n",
    "\n",
    "for final exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # definng neural network\n",
    "import torch.nn.functional as F # helper\n",
    "from torch.utils.data import DataLoader # batching\n",
    "from torchvision import datasets, transforms # loading datasets\n",
    "from torchvision.utils import make_grid #for visualization\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation package\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the version\n",
    "import torchvision\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1155c7350>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in deep learning, cannot do Cross Validation.\n",
    "# Instead, we can use seed -- to help serve as alternative cross-validation in deep learning --\n",
    "torch.manual_seed (47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some transforms (convert 0-255 to 0-1)\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# if you wan to crop the image, rotate the image; they are all done in this transform\n",
    "\n",
    "# use the pytorch datasets to load MNIST - dataset of digit images\n",
    "# train, test\n",
    "train_data = datasets.MNIST(root = 'data', train = True, download= True, transform = transform)\n",
    "\n",
    "# validation set\n",
    "train_set, val_set = torch.utils.data.random_split (train_data, [50000,10000])\n",
    "\n",
    "# test set\n",
    "test_data = datasets.MNIST (root='data', train = False, download= True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x105fec990>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigger batch size is prefer\n",
    "# if batch size is too big >> CUDA out of memory\n",
    "# buy more GPU or reduce the batch size\n",
    "train_loader = DataLoader (train_set, batch_size = 64, shuffle = True) # this shuffle will depend on our seed\n",
    "val_loader = DataLoader (val_set, batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader (test_data, batch_size = 10000, shuffle = False) # because no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader) # 782 * 64 = 50,048 = 50000 + 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape =  torch.Size([64, 1, 28, 28])\n",
      "label shape = torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_loader: # taking 64 images, corresponding 64 labels\n",
    "    break\n",
    "print(f\"images shape = \", images.shape)# (64 images, 1 channel, height, width)\n",
    "print (f\"label shape =\", labels.shape)\n",
    "# print (images)\n",
    "the_image = images[0] # get the first image\n",
    "the_image = np.transpose((the_image), (1,2,0)) # transporse the dimension\n",
    "the_image.shape\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(the_image)\n",
    "\n",
    "labels[0].item()\n",
    "# plt.imshow?? # (h,w,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape # input (bs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the relationship between the input an dconvolutional layer\n",
    "# conv1d  = time series, language, signal\n",
    "# conv2d = images, spectrograms\n",
    "\n",
    "# nn.Conv2d (1, 1, 3, 1, 1)\n",
    "layer1 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 3, stride = 1, padding = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to pass something into this layer\n",
    "output = layer1(images)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the loss and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
