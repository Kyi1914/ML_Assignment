{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch - CNN\n",
    "\n",
    "for final exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # definng neural network\n",
    "import torch.nn.functional as F # helper\n",
    "from torch.utils.data import DataLoader # batching\n",
    "from torchvision import datasets, transforms # loading datasets\n",
    "from torchvision.utils import make_grid #for visualization\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation package\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.0'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the version\n",
    "import torchvision\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1155c7350>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in deep learning, cannot do Cross Validation.\n",
    "# Instead, we can use seed -- to help serve as alternative cross-validation in deep learning --\n",
    "torch.manual_seed (47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some transforms (convert 0-255 to 0-1)\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# if you wan to crop the image, rotate the image; they are all done in this transform\n",
    "\n",
    "# use the pytorch datasets to load MNIST - dataset of digit images\n",
    "# train, test\n",
    "train_data = datasets.MNIST(root = 'data', train = True, download= True, transform = transform)\n",
    "\n",
    "# validation set\n",
    "train_set, val_set = torch.utils.data.random_split (train_data, [50000,10000])\n",
    "\n",
    "# test set\n",
    "test_data = datasets.MNIST (root='data', train = False, download= True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x29eaa6450>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigger batch size is prefer\n",
    "# if batch size is too big >> CUDA out of memory\n",
    "# buy more GPU or reduce the batch size\n",
    "train_loader = DataLoader (train_set, batch_size = 64, shuffle = True) # this shuffle will depend on our seed\n",
    "val_loader = DataLoader (val_set, batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader (test_data, batch_size = 10000, shuffle = False) # because no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader) # 782 * 64 = 50,048 = 50000 + 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape =  torch.Size([64, 1, 28, 28])\n",
      "label shape = torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_loader: # taking 64 images, corresponding 64 labels\n",
    "    break\n",
    "print(f\"images shape = \", images.shape)# (64 images, 1 channel, height, width)\n",
    "print (f\"label shape =\", labels.shape)\n",
    "# print (images)\n",
    "the_image = images[0] # get the first image\n",
    "the_image = np.transpose((the_image), (1,2,0)) # transporse the dimension\n",
    "the_image.shape\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(the_image)\n",
    "\n",
    "labels[0].item()\n",
    "# plt.imshow?? # (h,w,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape # input (bs, ch, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the relationship between the input an dconvolutional layer\n",
    "# conv1d  = time series, language, signal\n",
    "# conv2d = images, spectrograms\n",
    "\n",
    "# nn.Conv2d (1, 1, 3, 1, 1)\n",
    "# kernel_size = [3,3] >> 3, [5,5] >> 5\n",
    "layer1 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 3, stride = 1, padding = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to pass something into this layer\n",
    "output = layer1(images)\n",
    "output.shape # (bs, out-channel, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the output for each layer\n",
    "layer1 = nn.Conv2d (1, 5, 3, 1, 1)\n",
    "layer2 = nn.Conv2d (5, 10, 3, 1, 1)\n",
    "\n",
    "layer3 = nn.Linear (10 * 28 * 28, 64)\n",
    "\n",
    "out = layer2(layer1(images))\n",
    "out = out.reshape ((-1, 10 *28*28))\n",
    "out2 = layer3(out)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Chaky_CNN (nn.Module):\n",
    "    \n",
    "#     def __init__(self):  # init all hyperparameters and layers\n",
    "#         super().__init__() # inherit everything from nn.Module\n",
    "#         self.layer1 = nn.Conv2d (1, 5, 3, 1, 1)\n",
    "#         self.layer2 = nn.Conv2d (5, 10, 3, 1, 1) # note: here input layer is 5 as layer 1 output layer is 5\n",
    "#         self.layer3 = nn.Linear (10 * 28 * 28 , 120)\n",
    "#         self.layer4 = nn.Linear (120, 84) # why 120, 84 - chaky example\n",
    "#         self.layer5 = nn.Linear (84, 10) # why 10x10 classes - example\n",
    "    \n",
    "#     def forward(self, images): # performing the forward pass through all layers\n",
    "#         # images: 64, 1, 28, 28\n",
    "#         out = F.relu(self.layer1(images))\n",
    "#         # out: 64, 5, 28, 28\n",
    "#         out = F.relu(self.layer2(out))\n",
    "#         # out: 64, 10, 28, 28\n",
    "#         out.reshape((-1, 10*28*28))\n",
    "#         # out: 64, 7840\n",
    "#         out = F.relu(self.layer3(out))\n",
    "#         # out: 64, 120\n",
    "#         out = F.relu(self.layer4(out))\n",
    "#         # out: 64, 84\n",
    "#         out = F.relu(self.layer5(out))\n",
    "#         # out: 64, 10\n",
    "#         return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 0, 2])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is relu?\n",
    "numbers = torch.tensor ([1, 3, -9, 2])\n",
    "numbers\n",
    "F.relu(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chaky_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self): #init all hyperparameters and layers\n",
    "        super().__init__() #inherit everything from nn.Module\n",
    "        self.layer1 = nn.Conv2d(1, 5, 3, 1, 1) \n",
    "        self.layer2 = nn.Conv2d(5, 10, 3, 1, 1)\n",
    "        self.layer3 = nn.Linear(10 * 28 * 28, 120)\n",
    "        self.layer4 = nn.Linear(120, 84) #why 120, 84 - chaky examples\n",
    "        self.layer5 = nn.Linear(84, 10)  #why 10, 10 classes\n",
    "        \n",
    "    def forward(self, images):  #performing the forward pass through all layers\n",
    "        out = F.relu(self.layer1(images))\n",
    "        out = F.relu(self.layer2(out))\n",
    "        out = out.reshape((-1, 10*28*28))\n",
    "        out = F.relu(self.layer3(out))\n",
    "        out = F.relu(self.layer4(out))\n",
    "        out = F.relu(self.layer5(out))\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# test first\n",
    "\n",
    "chaky_model = Chaky_CNN()\n",
    "output      = chaky_model(images)\n",
    "print (output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip counting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(chaky_model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "# 1. loop epochs\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # 2. loop train loader\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        # 3. predict / forward pass\n",
    "        y_hat = chaky_model(images)\n",
    "        \n",
    "        # 4. loss\n",
    "        loss = criterion (y_hat, labels)\n",
    "        \n",
    "        # 5. calculate gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 6. update weight\n",
    "        optimizer.step()\n",
    "        # print(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8997066020965576"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss: 1.225827932357788\n"
     ]
    }
   ],
   "source": [
    "chaky_model.eval() # this will turn off dropout, batch norm\n",
    "with torch.no_grad(): # this will turn off gradient calculations\n",
    "    for images, labels in test_loader:\n",
    "        \n",
    "        # 1. predict\n",
    "        y_hat = chaky_model (images)\n",
    "        \n",
    "        # 2. loss\n",
    "        loss = criterion (y_hat, labels)\n",
    "        \n",
    "print(\"Cross Entropy Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 10]), torch.Size([10000]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_pred = torch.max (y_hat, 1)[1]\n",
    "y_hat_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 964, 1100,    5,    3,  884,  804,  938,    2,  889,   19],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   7,   10, 1002,    8,   16,    2,   15,   17,   10,    0],\n",
       "       [   6,   14,   16,  981,    0,   31,    4,    9,   21,    5],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   2,    8,    9,    5,   14,    5,    1,  983,    5,    8],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   1,    3,    0,   13,   68,   50,    0,   17,   49,  977]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix (y_hat_pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0].shape\n",
    "test_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29f168290>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.transpose(test_data[0][0],(1,2,0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = chaky_model(test_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  8.1751,  0.0000,  0.0000,  0.0000, 15.4453,\n",
       "          0.0000,  4.3424]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# among the probability of output number, 7 is the highest probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
