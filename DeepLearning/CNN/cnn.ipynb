{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch - CNN\n",
    "\n",
    "for final exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # definng neural network\n",
    "import torch.nn.functional as F # helper\n",
    "from torch.utils.data import DataLoader # batching\n",
    "from torchvision import datasets, transforms # loading datasets\n",
    "from torchvision.utils import make_grid #for visualization\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation package\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.0'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the version\n",
    "import torchvision\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1155c7350>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in deep learning, cannot do Cross Validation.\n",
    "# Instead, we can use seed -- to help serve as alternative cross-validation in deep learning --\n",
    "torch.manual_seed (47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some transforms (convert 0-255 to 0-1)\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# if you wan to crop the image, rotate the image; they are all done in this transform\n",
    "\n",
    "# use the pytorch datasets to load MNIST - dataset of digit images\n",
    "# train, test\n",
    "train_data = datasets.MNIST(root = 'data', train = True, download= True, transform = transform)\n",
    "\n",
    "# validation set\n",
    "train_set, val_set = torch.utils.data.random_split (train_data, [50000,10000])\n",
    "\n",
    "# test set\n",
    "test_data = datasets.MNIST (root='data', train = False, download= True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x29eaa6450>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigger batch size is prefer\n",
    "# if batch size is too big >> CUDA out of memory\n",
    "# buy more GPU or reduce the batch size\n",
    "train_loader = DataLoader (train_set, batch_size = 64, shuffle = True) # this shuffle will depend on our seed\n",
    "val_loader = DataLoader (val_set, batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader (test_data, batch_size = 10000, shuffle = False) # because no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader) # 782 * 64 = 50,048 = 50000 + 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape =  torch.Size([64, 1, 28, 28])\n",
      "label shape = torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_loader: # taking 64 images, corresponding 64 labels\n",
    "    break\n",
    "print(f\"images shape = \", images.shape)# (64 images, 1 channel, height, width)\n",
    "print (f\"label shape =\", labels.shape)\n",
    "# print (images)\n",
    "the_image = images[0] # get the first image\n",
    "the_image = np.transpose((the_image), (1,2,0)) # transporse the dimension\n",
    "the_image.shape\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(the_image)\n",
    "\n",
    "labels[0].item()\n",
    "# plt.imshow?? # (h,w,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape # input (bs, ch, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the relationship between the input an dconvolutional layer\n",
    "# conv1d  = time series, language, signal\n",
    "# conv2d = images, spectrograms\n",
    "\n",
    "# nn.Conv2d (1, 1, 3, 1, 1)\n",
    "# kernel_size = [3,3] >> 3, [5,5] >> 5\n",
    "layer1 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 3, stride = 1, padding = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to pass something into this layer\n",
    "output = layer1(images)\n",
    "output.shape # (bs, out-channel, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the output for each layer\n",
    "layer1 = nn.Conv2d (1, 5, 3, 1, 1)\n",
    "layer2 = nn.Conv2d (5, 10, 3, 1, 1)\n",
    "\n",
    "layer3 = nn.Linear (10 * 28 * 28, 64)\n",
    "\n",
    "out = layer2(layer1(images))\n",
    "out = out.reshape ((-1, 10 *28*28))\n",
    "out2 = layer3(out)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Chaky_CNN (nn.Module):\n",
    "    \n",
    "#     def __init__(self):  # init all hyperparameters and layers\n",
    "#         super().__init__() # inherit everything from nn.Module\n",
    "#         self.layer1 = nn.Conv2d (1, 5, 3, 1, 1)\n",
    "#         self.layer2 = nn.Conv2d (5, 10, 3, 1, 1) # note: here input layer is 5 as layer 1 output layer is 5\n",
    "#         self.layer3 = nn.Linear (10 * 28 * 28 , 120)\n",
    "#         self.layer4 = nn.Linear (120, 84) # why 120, 84 - chaky example\n",
    "#         self.layer5 = nn.Linear (84, 10) # why 10x10 classes - example\n",
    "    \n",
    "#     def forward(self, images): # performing the forward pass through all layers\n",
    "#         # images: 64, 1, 28, 28\n",
    "#         out = F.relu(self.layer1(images))\n",
    "#         # out: 64, 5, 28, 28\n",
    "#         out = F.relu(self.layer2(out))\n",
    "#         # out: 64, 10, 28, 28\n",
    "#         out.reshape((-1, 10*28*28))\n",
    "#         # out: 64, 7840\n",
    "#         out = F.relu(self.layer3(out))\n",
    "#         # out: 64, 120\n",
    "#         out = F.relu(self.layer4(out))\n",
    "#         # out: 64, 84\n",
    "#         out = F.relu(self.layer5(out))\n",
    "#         # out: 64, 10\n",
    "#         return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 0, 2])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is relu?\n",
    "numbers = torch.tensor ([1, 3, -9, 2])\n",
    "numbers\n",
    "F.relu(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chaky_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self): #init all hyperparameters and layers\n",
    "        super().__init__() #inherit everything from nn.Module\n",
    "        self.layer1 = nn.Conv2d(1, 5, 3, 1, 1) \n",
    "        self.layer2 = nn.Conv2d(5, 10, 3, 1, 1)\n",
    "        self.layer3 = nn.Linear(10 * 28 * 28, 120)\n",
    "        self.layer4 = nn.Linear(120, 84) #why 120, 84 - chaky examples\n",
    "        self.layer5 = nn.Linear(84, 10)  #why 10, 10 classes\n",
    "        \n",
    "    def forward(self, images):  #performing the forward pass through all layers\n",
    "        out = F.relu(self.layer1(images))\n",
    "        out = F.relu(self.layer2(out))\n",
    "        out = out.reshape((-1, 10*28*28))\n",
    "        out = F.relu(self.layer3(out))\n",
    "        out = F.relu(self.layer4(out))\n",
    "        out = F.relu(self.layer5(out))\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# test first\n",
    "\n",
    "chaky_model = Chaky_CNN()\n",
    "output      = chaky_model(images)\n",
    "print (output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip counting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the loss and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
