{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../ANN/japan.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nm = 5, n = 3(Temp, Rainfall, Humidity)\\ny = yield of Apples, Orange\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "m = 5, n = 3(Temp, Rainfall, Humidity)\n",
    "y = yield of Apples, Orange\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Specify X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "X_train = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "Y_train = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# have to convert numpy to torch tensors\n",
    "# torch tensors are simply numpy version of pytorch\n",
    "inputs  = torch.from_numpy(X_train)\n",
    "targets = torch.from_numpy(Y_train)\n",
    "\n",
    "print(type(inputs))\n",
    "print(type(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 3]), torch.Size([15, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, targets.shape\n",
    "inputs.size(), targets.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember in ML - we have to do the batch learning ourselve, like mini-batch, stochastic batch. \n",
    "Pytorch has a class called DataLoaders that automatically do this for you.\n",
    "\n",
    "It's optional whether you wanna use it or just use it - don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([73., 67., 43.]), tensor([56., 70.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a Dataset so that dataloaders understand\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(inputs, targets)\n",
    "\n",
    "train_dataset[0]\n",
    "# get the 3 samples from the dataset\n",
    "# train_dataset[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 3 # for no reason - here I am using a mini-batch of size 3\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle = True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----new batch----\n",
      "tensor([[102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [ 87., 134.,  58.]])\n",
      "tensor([[ 22.,  37.],\n",
      "        [103., 119.],\n",
      "        [119., 133.]])\n",
      "----new batch----\n",
      "tensor([[ 91.,  88.,  64.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 87., 134.,  58.]])\n",
      "tensor([[ 81., 101.],\n",
      "        [ 56.,  70.],\n",
      "        [119., 133.]])\n",
      "----new batch----\n",
      "tensor([[ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.]])\n",
      "tensor([[ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.]])\n",
      "----new batch----\n",
      "tensor([[69., 96., 70.],\n",
      "        [73., 67., 43.],\n",
      "        [91., 88., 64.]])\n",
      "tensor([[103., 119.],\n",
      "        [ 56.,  70.],\n",
      "        [ 81., 101.]])\n",
      "----new batch----\n",
      "tensor([[ 73.,  67.,  43.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dl:\n",
    "    print(\"----new batch----\")\n",
    "    print(x) # this will give you \"batch size \" of x, e.g. 3 sets of x\n",
    "    print(y) # this will give you \"batch size \" of y, e.g. 3 sets of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define some layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be confuse about \"Layers\". it's simply matrix multiplication. It's true until now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # nn = neural nework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model - linear regression\n",
    "some_random_layer = nn.Linear(3,2) # 3 = incoming features, 2 = output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0513, -0.2395,  0.5170],\n",
      "        [-0.0656, -0.1019, -0.3464]], requires_grad=True)\n",
      "torch.Size([2, 3])\n",
      "Parameter containing:\n",
      "tensor([ 0.0084, -0.4997], requires_grad=True)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(some_random_layer.weight) # matrix - randomly initialize --> using standardization methods.\n",
    "print(some_random_layer.weight.shape)\n",
    "print(some_random_layer.bias)\n",
    "print(some_random_layer.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: output features is 2. So, bias has 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict \n",
    "\n",
    "outputs = some_random_layer(inputs)\n",
    "# input = (15,3) @ (3,2) = (15, 2)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0513, -0.2395,  0.5170],\n",
      "        [-0.0656, -0.1019, -0.3464]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0084, -0.4997], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# how do chatgpt count their number of parameters - 175b parameters\n",
    "\n",
    "total_num_of_params = 0\n",
    "for param in some_random_layer.parameters():\n",
    "    print(param)\n",
    "    # total_num_of_params+="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(param)\n",
    "\n",
    "    Parameter containing: [the first parameters, w = 6]\n",
    "    tensor([[ 0.5407,  0.1850, -0.3178],\n",
    "            [ 0.1833,  0.2263,  0.1200]], requires_grad=True) \n",
    "    \n",
    "    Parameter containing: [the second parameters, bias = 2]\n",
    "    tensor([-0.2637,  0.3207], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for param in some_random_layer.parameters():\n",
    "    print(param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "total_num_of_params = 0\n",
    "for param in some_random_layer.parameters():\n",
    "    total_num_of_params += param.numel()\n",
    "    \n",
    "print(total_num_of_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall that this is regression problem\n",
    "# so we have to use MSE\n",
    "# you can code by yourself - or you can cuse built-in pytorch module\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = criterion (targets, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11465.2617, grad_fn=<MseLossBackward0>)\n",
      "11465.26171875\n"
     ]
    }
   ],
   "source": [
    "print(mse)\n",
    "print(mse.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define our gradient descent algorithm\n",
    "\n",
    "Recall we use the gradient descent algorithm w = w- alpha * gradient\n",
    "\n",
    "In fact, there are more, momentum, Adam --> adaptive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(some_random_layer.parameters(), lr = 0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting them together - actually Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1043.6965, -1454.0029,  -813.3240],\n",
      "        [-4466.4971, -6062.3994, -3464.6118]])\n",
      "tensor([[ 3144.7061,  3688.3086,  1952.3918],\n",
      "        [15881.3496, 18912.5332,  9895.5957]])\n",
      "tensor([[1892.9827, 2048.0273, 1317.4324],\n",
      "        [8493.6543, 9415.7656, 5896.7383]])\n",
      "tensor([[ -2432.1868,  -2473.5774,  -1777.0806],\n",
      "        [-12372.4014, -12311.7236,  -8849.4268]])\n",
      "tensor([[ -2446.6692,  -2205.6978,  -1676.7224],\n",
      "        [-11635.2441, -10338.7871,  -7872.5747]])\n",
      "Epoch: 0 | Loss: 13086.882812\n",
      "tensor([[ -556.9272,  -930.1807,  -416.9375],\n",
      "        [-1669.1648, -2973.3335, -1227.3447]])\n",
      "tensor([[ 5730.7666,  5081.5713,  3186.3804],\n",
      "        [26425.4648, 23417.2910, 14635.0010]])\n",
      "tensor([[  -73.6984,  -154.0888,  -115.4095],\n",
      "        [-1466.7733, -1893.8452, -1369.2241]])\n",
      "tensor([[ -3157.6460,  -3124.6079,  -2246.1567],\n",
      "        [-13864.6104, -14006.6562, -10034.7520]])\n",
      "tensor([[ -2791.9802,  -3587.3052,  -1720.7606],\n",
      "        [-11698.4189, -15605.0293,  -7379.4551]])\n",
      "Epoch: 1 | Loss: 16743.162109\n",
      "tensor([[ 4129.2754,  4109.6851,  2580.3521],\n",
      "        [18720.5410, 18213.2910, 11521.3662]])\n",
      "tensor([[ 4335.8320,  3395.8247,  2551.7402],\n",
      "        [18807.1934, 14463.0684, 10901.9512]])\n",
      "tensor([[ -4681.4521,  -5680.9331,  -3135.2612],\n",
      "        [-20547.5234, -25330.1602, -13908.3457]])\n",
      "tensor([[ -2264.2595,  -2870.8879,  -1432.6078],\n",
      "        [-10130.6162, -12801.4141,  -6404.2837]])\n",
      "tensor([[ 4076.6782,  4487.1260,  3192.2002],\n",
      "        [18395.5723, 20296.1758, 14442.8740]])\n",
      "Epoch: 2 | Loss: 30264.083984\n",
      "tensor([[ 2716.9751,  3382.1311,  2464.4192],\n",
      "        [12342.3066, 15424.6074, 11239.5674]])\n",
      "tensor([[ -6081.9189,  -3189.4998,  -2491.4521],\n",
      "        [-27191.8164, -14359.2324, -11184.3438]])\n",
      "tensor([[ -4981.9473,  -7673.3438,  -3321.2981],\n",
      "        [-23261.9727, -35828.7852, -15507.9824]])\n",
      "tensor([[ 6636.5903,  6240.3184,  4255.5352],\n",
      "        [29950.9258, 28163.0000, 19206.2656]])\n",
      "tensor([[ 6583.2686,  6346.0488,  4716.5859],\n",
      "        [31254.6523, 30071.1641, 22359.8164]])\n",
      "Epoch: 3 | Loss: 75594.914062\n",
      "tensor([[ -5534.3662,  -3992.6055,  -2943.6099],\n",
      "        [-25445.8516, -18424.0977, -13570.9717]])\n",
      "tensor([[ -8909.7344,  -8430.8555,  -5080.9131],\n",
      "        [-41258.4375, -38931.6094, -23531.7930]])\n",
      "tensor([[ 3729.5146,  4657.0444,  3305.6851],\n",
      "        [17164.4570, 21533.1250, 15303.5908]])\n",
      "tensor([[ 7562.6089,  5866.3643,  4248.5469],\n",
      "        [33857.6328, 26256.4121, 19017.4004]])\n",
      "tensor([[ 1242.8219,  1946.7654,   752.5624],\n",
      "        [ 7243.5923, 11244.8955,  4623.3013]])\n",
      "Epoch: 4 | Loss: 6419.875000\n"
     ]
    }
   ],
   "source": [
    "# basically same as Linear and Logistic Regeression\n",
    "num_epoch = 5\n",
    "# 1. loop epoch\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    # 2. loop the mini batch of samples (Dataloaders are using without-replacement)\n",
    "    for batch_x, batch_y in train_dl:\n",
    "\n",
    "        #optional: put your data into GPU using .device()\n",
    "        \n",
    "        # 2.1 Predict\n",
    "        yhat = some_random_layer(batch_x)\n",
    "        \n",
    "        # 2.2 Calculate Loss\n",
    "        loss = criterion(yhat,batch_y)\n",
    "        \n",
    "        # 2.3 Calculate Gradients\n",
    "        optimizer.zero_grad() # clear all the gradient\n",
    "        loss.backward() # this will calculate all the gradient in respect to loss\n",
    "        \n",
    "        # print the gradient\n",
    "        # print(some_random_layer.weight.grad)\n",
    "        # if most of the thing are in 1000, lr should be around 0.0001.\n",
    "        # print the old weight\n",
    "        # print(f\"Original w: {some_random_layer.weight:.3f}\")\n",
    "        \n",
    "        # 2.4 Update! --> Backpropagate\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print updated weight\n",
    "        # print(f\"Original w: {some_random_layer.weight:.3f}\")\n",
    "        \n",
    "    # print the summary of each epoch\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
